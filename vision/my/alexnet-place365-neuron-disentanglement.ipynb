{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confidential-dubai",
   "metadata": {},
   "source": [
    "# AlexNet Places 365 Neuron Disentanglement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/notebooks/compexp/vision/')\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "\n",
    "# conexp\n",
    "sys.path.append('')\n",
    "import settings\n",
    "from loader.model_loader import loadmodel\n",
    "\n",
    "\n",
    "sys.path.append('./my')\n",
    "from rosettastone import maxact\n",
    "from rosettastone.disentanglement import disentanglenet, splitting_operation, succeeding_operation\n",
    "from rosettastone.utils import InfiniteDataLoader, freezer, forward_up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-halifax",
   "metadata": {},
   "source": [
    "## Data & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loadmodel(None)\n",
    "\n",
    "def normalize_image(rgb_image):\n",
    "    img = np.array(rgb_image, dtype=np.float32).copy()\n",
    "    img = img[:, ::-1]\n",
    "    img -= [109.5388, 118.6897, 124.6901]\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    \n",
    "    # accuracy is better without rgb -> bgr, not clear why it is necessery\n",
    "    # perhaps it is necesseray for ADE20K? Don't know right now.\n",
    "    # img = torch.from_numpy(img[::-1, :, :].copy())\n",
    "    \n",
    "    img = torch.from_numpy(img.copy())\n",
    "    img = img.div_(255.0 * 0.224)\n",
    "    \n",
    "    return img\n",
    "\n",
    "place365_dataset = torchvision.datasets.Places365(root='/notebooks/compexp/vision/my/data', split='val', small=True,\n",
    "                                         transform=transforms.Lambda(normalize_image))\n",
    "place365_dataloader = torch.utils.data.DataLoader(place365_dataset, batch_size=32, num_workers=8, shuffle=False)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-google",
   "metadata": {},
   "source": [
    "## Neuron on the Table: Last Conv2D layer @ 31\n",
    "## Polysemantic: Buildings & People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURON = 31\n",
    "TOP_K = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    orig_activations = maxact.harvest_activations(model, place365_dataloader, {'last_conv': lambda m: m.features[10]})\n",
    "\n",
    "maxact.visualize_neuron('last_conv', NEURON, model, orig_activations, place365_dataset,\n",
    "                        with_cropped=False, with_lucent=False, top_k=TOP_K)\n",
    "    \n",
    "(top_img_acts,\n",
    " top_img_indice,\n",
    " _,\n",
    " top_reprs_last_conv,\n",
    " _, _) = maxact.get_neuron_max_activations(orig_activations, place365_dataset, 'last_conv', NEURON, top_k=TOP_K)\n",
    "\n",
    "top_last_conv_repers = (PCA(n_components=2, random_state=0)\n",
    "                        .fit_transform(orig_activations['last_conv'][top_img_indice, :]\n",
    "                                       .mean(dim=(2, 3))))\n",
    "\n",
    "del orig_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-region",
   "metadata": {},
   "source": [
    "### Manual labeling of the Top-50 max activiations images to **concepts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually extracted\n",
    "y_concept = np.array(['building'] * 9 + ['people'] * 2\n",
    "                     + ['building'] + ['people'] + ['building']\n",
    "                     + ['people']*3 + ['building'] * 3\n",
    "                     + ['other'] + ['people'] + ['building'] * 3\n",
    "                     + ['people'] * 7 + ['building'] * 3 + ['people']\n",
    "                     + ['building'] * 3 + ['people'] +['building'] * 2\n",
    "                     + ['people'] * 2 + ['building'] * 3 + ['people'] * 2 + ['building'])\n",
    "\n",
    "\n",
    "first_concept = 'building'\n",
    "second_concept = 'people'\n",
    "\n",
    "concepts = {first_concept, second_concept}\n",
    "\n",
    "concept_datasets = {c: Subset(place365_dataset, top_img_indice[y_concept == c])\n",
    "                    for c in concepts}\n",
    "\n",
    "concept_dataloaders = {c: InfiniteDataLoader(ds, batch_size=8)\n",
    "                      for c, ds in concept_datasets.items()}\n",
    "\n",
    "concept_probs = {c: DataLoader(ds, batch_size=len(ds))\n",
    "                for c, ds in concept_datasets.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-gateway",
   "metadata": {},
   "source": [
    "### Representations and Seperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-royalty",
   "metadata": {},
   "source": [
    "#### Are the concepts linearly separated in the last conv layer representation? Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=top_last_conv_repers[:, 0], y=top_last_conv_repers[:, 1], hue=y_concept);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-assurance",
   "metadata": {},
   "source": [
    "#### Are the concepts linearly separated in the penultimate conv layer representation? Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    orig_activations = maxact.harvest_activations(model, place365_dataloader, {'penultimate_conv': lambda m: m.features[9]},\n",
    "                                                 apply_fn=lambda x:x.mean(dim=(2,3)))\n",
    "    \n",
    "top_penultimate_conv_repers = PCA(n_components=2, random_state=0).fit_transform(orig_activations['penultimate_conv'][top_img_indice, :])\n",
    "\n",
    "del orig_activations\n",
    "\n",
    "sns.scatterplot(x=top_penultimate_conv_repers[:, 0], y=top_penultimate_conv_repers[:, 1], hue=y_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-sierra",
   "metadata": {},
   "source": [
    "#### Are the concepts linearly separated in the first conv layer representation? Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    orig_activations = maxact.harvest_activations(model, place365_dataloader, {'first_conv': lambda m: m.features[0]},\n",
    "                                                  apply_fn=lambda x:x.mean(dim=(2,3)))\n",
    "    \n",
    "top_first_conv_repers = PCA(n_components=2, random_state=0).fit_transform(orig_activations['first_conv'][top_img_indice, :])\n",
    "\n",
    "del orig_activations\n",
    "\n",
    "sns.scatterplot(x=top_first_conv_repers[:, 0], y=top_first_conv_repers[:, 1], hue=y_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-deadline",
   "metadata": {},
   "source": [
    "## Define `DisentangleNet` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexDisentangleNet(nn.Module):\n",
    "    def __init__(self, net, neuron, new_neuron_noise_std, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.orig_net = copy.deepcopy(net)\n",
    "        self.splitted_net = copy.deepcopy(net)\n",
    "        \n",
    "        self.neuron = neuron\n",
    "        self.device = device\n",
    "        self.new_neuron_noise_std = new_neuron_noise_std\n",
    "        \n",
    "        self.orig_layer_forward_fn = None\n",
    "        self.splitted_layer_forward_fn = None\n",
    "        \n",
    "        self.layer_getter = None\n",
    "\n",
    "        self.active_net = 'orig'\n",
    "        \n",
    "        self._prepare_splitted()\n",
    "\n",
    "        \n",
    "    def _prepare_splitted(self):\n",
    "        \n",
    "        freezer(self.orig_net)\n",
    "        freezer(self.splitted_net)\n",
    "\n",
    "        self.splitted_net.features[10], _ = splitting_operation(self.orig_net.features[10],\n",
    "                                                             self.neuron,\n",
    "                                                             self.new_neuron_noise_std,\n",
    "                                                             self.device)\n",
    "        \n",
    "        self.splitted_net.classifier[1], _ = succeeding_operation(self.orig_net.classifier[1],\n",
    "                                                             self.neuron,\n",
    "                                                             self.orig_net.features[10].out_channels,\n",
    "                                                             self.device)\n",
    "\n",
    "        self.orig_layer_forward_fn = forward_up_to(self.orig_net.features, 10)\n",
    "        self.splitted_layer_forward_fn = forward_up_to(self.splitted_net.features, 10)\n",
    "        self.layer_getter = lambda m: m.features[10]\n",
    "\n",
    "        self.eval()  # turn off dropout, but also BN and it is not good\n",
    "\n",
    "        \n",
    "    def forward(self, x, active_net=None):\n",
    "        if active_net is None:\n",
    "            active_net = self.active_net\n",
    "        assert active_net in ('orig', 'splitted')\n",
    "        return self.orig_net(x) if active_net == 'orig' else self.splitted_net(x)\n",
    "\n",
    "    \n",
    "    def generate_losses(self, x_indff, x_cnpt1, x_cnpt2):\n",
    "        x_indff_orig = self.orig_net.features(x_indff)\n",
    "        x_indff_orig = self.orig_net.avgpool(x_indff_orig)\n",
    "        x_indff_orig = torch.flatten(x_indff_orig, 1)\n",
    "        x_indff_orig = self.orig_net.classifier[1](x_indff_orig)  # skip dropout\n",
    "\n",
    "        x_indff_splitted = self.splitted_net.features(x_indff)\n",
    "        x_indff_splitted = self.splitted_net.avgpool(x_indff_splitted)\n",
    "        x_indff_splitted = torch.flatten(x_indff_splitted, 1)\n",
    "        x_indff_splitted = self.splitted_net.classifier[1](x_indff_splitted) # skip dropout\n",
    "\n",
    "        indff_loss = (torch.flatten(F.mse_loss(x_indff_orig,\n",
    "                                         x_indff_splitted,\n",
    "                                                 reduction='none'), 1))    \n",
    "        x_cnpt1 = self.splitted_layer_forward_fn(x_cnpt1)\n",
    "        x_cnpt2 = self.splitted_layer_forward_fn(x_cnpt2)\n",
    "        \n",
    "        spc11_loss = (x_cnpt1[:, self.neuron, :, :]**2).mean(dim=(1, 2))\n",
    "        spc12_loss = (x_cnpt1[:, -1, :, :]**2).mean(dim=(1, 2))\n",
    "\n",
    "        spc21_loss = (x_cnpt2[:, self.neuron, :, :]**2).mean(dim=(1, 2))\n",
    "        spc22_loss = (x_cnpt2[:, -1, :, :]**2).mean(dim=(1, 2))\n",
    "\n",
    "        wd1_loss = (self.splitted_net.features[10].weight[self.neuron, :, :, :]**2).mean()\n",
    "        wd2_loss = (self.splitted_net.features[10].weight[-1, :, :, :]**2).mean()\n",
    "        \n",
    "        return (indff_loss,\n",
    "                spc11_loss,\n",
    "                spc22_loss,\n",
    "                spc12_loss,\n",
    "                spc21_loss,\n",
    "                wd1_loss,\n",
    "                wd2_loss)\n",
    "\n",
    "    def equality_report(self):\n",
    "        print()\n",
    "        print('==', (self.orig_net.features[10].bias[:self.neuron] == self.splitted_net.features[10].bias[:self.neuron]).all())\n",
    "        print('==', (self.orig_net.features[10].bias[self.neuron+1:] == self.splitted_net.features[10].bias[self.neuron+1:-1]).all())\n",
    "        print('==', (self.orig_net.features[10].bias[self.neuron] == self.splitted_net.features[10].bias[self.neuron]).all())\n",
    "        print('==', (self.orig_net.features[10].bias[self.neuron] == self.splitted_net.features[10].bias[-1]).all())\n",
    "        print()\n",
    "        if self.neuron:\n",
    "            print('==', (self.orig_net.classifier[1].weight[:, :36*self.neuron] ==  self.splitted_net.classifier[1].weight[:, :36*self.neuron]).any())\n",
    "        print('==', (self.orig_net.classifier[1].weight[:, 36*(self.neuron+1):] ==  self.splitted_net.classifier[1].weight[:, 36*(self.neuron+1):-36]).any())\n",
    "        print('!=', (self.orig_net.classifier[1].weight[:, 36*self.neuron:36*(self.neuron+1)] ==  self.splitted_net.classifier[1].weight[:, 36*self.neuron:36*(self.neuron+1)]).any())\n",
    "        print('!=', (self.splitted_net.classifier[1].weight[:, 36*self.neuron:36*(self.neuron+1)] ==  self.splitted_net.classifier[1].weight[:, -36:]).any())\n",
    "        print('==', (self.orig_net.classifier[1].bias ==  self.splitted_net.classifier[1].bias).any())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-accused",
   "metadata": {},
   "source": [
    "## Run the Disentanglement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "disentanglenet(AlexDisentangleNet, model, NEURON,\n",
    "               place365_dataset, place365_dataloader, place365_dataloader,\n",
    "               first_concept, second_concept, concept_probs, concept_dataloaders,\n",
    "               epochs=1, lr=1e-3, alpha=1e-3, beta=1e-2, new_neuron_noise_std=1e-4,\n",
    "               device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-jason",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random', #grid, random\n",
    "    'metric': {\n",
    "      'name': 'test_accuracy_splitted',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs': {\n",
    "            'values': [1, 2, 5, 10]\n",
    "        },\n",
    "        'alpha': {\n",
    "            'values': [1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5]\n",
    "        },\n",
    "        'beta': {\n",
    "            'values': [1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5]\n",
    "        },\n",
    "        'lr': {\n",
    "            'values': [1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5]\n",
    "        },\n",
    "        'new_neuron_noise_std': {\n",
    "            'values': [1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5, 0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"disentanglement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_trainer():\n",
    "    disentanglenet(AlexDisentangleNet, model, NEURON,\n",
    "                   place365_dataset, place365_dataloader, place365_dataloader,\n",
    "                   first_concept, second_concept, concept_probs, concept_dataloaders,\n",
    "                   epochs=1, lr=1e-3, alpha=1e-3, beta=1e-2, new_neuron_noise_std=1e-4,\n",
    "                   verbose=False,\n",
    "                   device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, sweep_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-warner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-bacon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
